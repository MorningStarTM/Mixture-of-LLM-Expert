{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\github_clone\\\\Mixture-of-LLM-Expert'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from MoE import SparseMoELanguageModel, kaiming_init_weights\n",
    "from Utils import model_params\n",
    "from Tokenizer import WordLevelTokenizer, extract_and_save_text, LetterLevelTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "max_iters = 100000\n",
    "learning_rate = 3e-4\n",
    "block_size = 8\n",
    "batch_size = 8\n",
    "eval_iters = 500\n",
    "n_emb = 384\n",
    "n_layers = 6\n",
    "n_head = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'data\\\\newspaper-text-summarization-cnn-dailymail\\\\cnn_dailymail\\\\test.csv'  # Replace with your CSV file path\n",
    "output_directory = 'data\\\\'  # Replace with your desired output directory\n",
    "\n",
    "extract_and_save_text(csv_file_path, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\\\summaries.txt\", \"rb\") as txt:\n",
    "    texts = txt.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz £¥°´·½ÂáãäåçèéëíîïñóöûüćčēłŠšž​‎–—‘’“”…€\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(texts.decode())))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LetterLevelTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [34, 12, 94, 19, 7, 4, 94, 41, 17, 4, 18, 8, 3, 4, 13, 19, 94, 14, 5, 94, 19, 7, 4, 94, 26, 12, 4, 17, 8, 2, 0, 75]\n",
      "Detokenized text: Im the President of the America.\n"
     ]
    }
   ],
   "source": [
    "temp = \"Im the President of the America.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(temp)\n",
    "print(\"Tokens:\", tokens)  # Output: Tokens: [index values representing each word]\n",
    "\n",
    "original_text = tokenizer.detokenize(tokens)\n",
    "print(\"Detokenized text:\", original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.tokenize(texts.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3545403]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "encoded_data = torch.tensor(data, dtype=torch.long)\n",
    "print(encoded_data.shape, encoded_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens : 3190862  --- Validation tokens : 354541\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(encoded_data))\n",
    "train_data = encoded_data[:n]\n",
    "val_data = encoded_data[n:]\n",
    "\n",
    "print(f\"Training tokens : {len(train_data)}  --- Validation tokens : {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([30]) the target: 23\n",
      "when input is tensor([30, 23]) the target: 15\n",
      "when input is tensor([30, 23, 15]) the target: 4\n",
      "when input is tensor([30, 23, 15,  4]) the target: 17\n",
      "when input is tensor([30, 23, 15,  4, 17]) the target: 19\n",
      "when input is tensor([30, 23, 15,  4, 17, 19]) the target: 18\n",
      "when input is tensor([30, 23, 15,  4, 17, 19, 18]) the target: 94\n",
      "when input is tensor([30, 23, 15,  4, 17, 19, 18, 94]) the target: 16\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "tensor([[ 6, 17, 24, 68, 73, 94,  2, 14],\n",
      "        [ 4,  4, 13, 94, 19, 14, 94, 21],\n",
      "        [13,  3, 18,  7,  8, 15, 94,  2],\n",
      "        [11,  8, 13,  6, 94, 58, 74, 24],\n",
      "        [ 7, 17,  4,  4, 94,  0, 11, 11],\n",
      "        [24, 94,  8, 13, 94,  2, 14, 13],\n",
      "        [ 3, 94, 14, 19,  7,  4, 17, 94],\n",
      "        [ 3,  4, 94,  8, 13, 94, 48,  8]])\n",
      "Targets: \n",
      "tensor([[17, 24, 68, 73, 94,  2, 14, 20],\n",
      "        [ 4, 13, 94, 19, 14, 94, 21,  8],\n",
      "        [ 3, 18,  7,  8, 15, 94,  2,  7],\n",
      "        [ 8, 13,  6, 94, 58, 74, 24,  4],\n",
      "        [17,  4,  4, 94,  0, 11, 11,  4],\n",
      "        [94,  8, 13, 94,  2, 14, 13, 13],\n",
      "        [94, 14, 19,  7,  4, 17, 94,  0],\n",
      "        [ 4, 94,  8, 13, 94, 48,  8, 13]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(encoded_data) - block_size, (batch_size,))\n",
    "    x = torch.stack([encoded_data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([encoded_data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"Inputs: \")\n",
    "print(xb)\n",
    "print(\"Targets: \")\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMoELanguageModel(\n",
       "  (token_embedding_table): Embedding(129, 368)\n",
       "  (position_embedding_table): Embedding(32, 368)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x Head(\n",
       "            (key): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (query): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (value): Linear(in_features=368, out_features=46, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=368, out_features=368, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (smoe): SparseMoE(\n",
       "        (router): NoisyTopkRouter(\n",
       "          (topkroute_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "          (noise_linear): Linear(in_features=368, out_features=8, bias=True)\n",
       "        )\n",
       "        (experts): ModuleList(\n",
       "          (0-7): 8 x Expert(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=368, out_features=1472, bias=True)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=1472, out_features=368, bias=True)\n",
       "              (3): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((368,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=368, out_features=129, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SparseMoELanguageModel(vocab_size)\n",
    "model.apply(kaiming_init_weights)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 73,957,953\n",
      "Trainable parameters: 73,957,953\n",
      "Non-trainable parameters: 0\n"
     ]
    }
   ],
   "source": [
    "model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 0 train loss: 5.77391242980957 val loss: 5.754223346710205\n",
      "steps: 500 train loss: 2.616345167160034 val loss: 2.6125969886779785\n",
      "steps: 1000 train loss: 2.4855411052703857 val loss: 2.469041347503662\n",
      "steps: 1500 train loss: 2.386364698410034 val loss: 2.3921549320220947\n",
      "steps: 2000 train loss: 2.356753349304199 val loss: 2.3495171070098877\n",
      "steps: 2500 train loss: 2.3321874141693115 val loss: 2.3349690437316895\n",
      "steps: 3000 train loss: 2.28495192527771 val loss: 2.2820966243743896\n",
      "steps: 3500 train loss: 2.2662851810455322 val loss: 2.2679355144500732\n",
      "steps: 4000 train loss: 2.226921319961548 val loss: 2.2384283542633057\n",
      "steps: 4500 train loss: 2.229139804840088 val loss: 2.22273850440979\n",
      "steps: 5000 train loss: 2.1903066635131836 val loss: 2.209552049636841\n",
      "steps: 5500 train loss: 2.1668858528137207 val loss: 2.1823906898498535\n",
      "steps: 6000 train loss: 2.157639265060425 val loss: 2.17037296295166\n",
      "steps: 6500 train loss: 2.132796049118042 val loss: 2.1502015590667725\n",
      "steps: 7000 train loss: 2.1404902935028076 val loss: 2.1238787174224854\n",
      "steps: 7500 train loss: 2.120889663696289 val loss: 2.130629777908325\n",
      "steps: 8000 train loss: 2.1070213317871094 val loss: 2.116962194442749\n",
      "steps: 8500 train loss: 2.0814218521118164 val loss: 2.099477529525757\n",
      "steps: 9000 train loss: 2.0842297077178955 val loss: 2.08455491065979\n",
      "steps: 9500 train loss: 2.073845863342285 val loss: 2.090855598449707\n",
      "steps: 10000 train loss: 2.0716683864593506 val loss: 2.0841386318206787\n",
      "steps: 10500 train loss: 2.077975273132324 val loss: 2.0793309211730957\n",
      "steps: 11000 train loss: 2.0721547603607178 val loss: 2.0612330436706543\n",
      "steps: 11500 train loss: 2.0499355792999268 val loss: 2.0664689540863037\n",
      "steps: 12000 train loss: 2.0577657222747803 val loss: 2.0550663471221924\n",
      "steps: 12500 train loss: 2.057342767715454 val loss: 2.0483040809631348\n",
      "steps: 13000 train loss: 2.043828010559082 val loss: 2.0493340492248535\n",
      "steps: 13500 train loss: 2.0498383045196533 val loss: 2.0327486991882324\n",
      "steps: 14000 train loss: 2.0198910236358643 val loss: 2.0278031826019287\n",
      "steps: 14500 train loss: 2.0176050662994385 val loss: 2.0163087844848633\n",
      "steps: 15000 train loss: 2.0181539058685303 val loss: 2.0275444984436035\n",
      "steps: 15500 train loss: 2.0036118030548096 val loss: 2.0063178539276123\n",
      "steps: 16000 train loss: 1.9957005977630615 val loss: 1.9895507097244263\n",
      "steps: 16500 train loss: 1.9968754053115845 val loss: 2.003976821899414\n",
      "steps: 17000 train loss: 1.9783143997192383 val loss: 2.0031094551086426\n",
      "steps: 17500 train loss: 1.9782334566116333 val loss: 1.9852938652038574\n",
      "steps: 18000 train loss: 1.9789648056030273 val loss: 1.9781981706619263\n",
      "steps: 18500 train loss: 1.9867937564849854 val loss: 1.9872287511825562\n",
      "steps: 19000 train loss: 1.983302354812622 val loss: 1.9670829772949219\n",
      "steps: 19500 train loss: 1.9898685216903687 val loss: 1.9879180192947388\n",
      "steps: 20000 train loss: 1.9933850765228271 val loss: 1.98308265209198\n",
      "steps: 20500 train loss: 1.9658790826797485 val loss: 1.9793094396591187\n",
      "steps: 21000 train loss: 1.9728530645370483 val loss: 1.9902634620666504\n",
      "steps: 21500 train loss: 1.9655708074569702 val loss: 1.9453235864639282\n",
      "steps: 22000 train loss: 1.959421157836914 val loss: 1.9566482305526733\n",
      "steps: 22500 train loss: 1.9618428945541382 val loss: 1.9726362228393555\n",
      "steps: 23000 train loss: 1.9564235210418701 val loss: 1.9445711374282837\n",
      "steps: 23500 train loss: 1.9542142152786255 val loss: 1.9446696043014526\n",
      "steps: 24000 train loss: 1.9573712348937988 val loss: 1.9490801095962524\n",
      "steps: 24500 train loss: 1.9512269496917725 val loss: 1.949750542640686\n",
      "steps: 25000 train loss: 1.9539705514907837 val loss: 1.9441200494766235\n",
      "steps: 25500 train loss: 1.9482554197311401 val loss: 1.9426604509353638\n",
      "steps: 26000 train loss: 1.937039852142334 val loss: 1.9482769966125488\n",
      "steps: 26500 train loss: 1.9324012994766235 val loss: 1.9461790323257446\n",
      "steps: 27000 train loss: 1.924411654472351 val loss: 1.9308998584747314\n",
      "steps: 27500 train loss: 1.9432731866836548 val loss: 1.929604411125183\n",
      "steps: 28000 train loss: 1.9306433200836182 val loss: 1.9261339902877808\n",
      "steps: 28500 train loss: 1.9390652179718018 val loss: 1.9271522760391235\n",
      "steps: 29000 train loss: 1.9247685670852661 val loss: 1.9173086881637573\n",
      "steps: 29500 train loss: 1.9350727796554565 val loss: 1.9342252016067505\n",
      "steps: 30000 train loss: 1.9300836324691772 val loss: 1.9238009452819824\n",
      "steps: 30500 train loss: 1.9152257442474365 val loss: 1.923292875289917\n",
      "steps: 31000 train loss: 1.9128806591033936 val loss: 1.9097778797149658\n",
      "steps: 31500 train loss: 1.9101073741912842 val loss: 1.909169316291809\n",
      "steps: 32000 train loss: 1.9098471403121948 val loss: 1.9165539741516113\n",
      "steps: 32500 train loss: 1.9145445823669434 val loss: 1.9035896062850952\n",
      "steps: 33000 train loss: 1.9177926778793335 val loss: 1.9289361238479614\n",
      "steps: 33500 train loss: 1.9220688343048096 val loss: 1.906592845916748\n",
      "steps: 34000 train loss: 1.8996831178665161 val loss: 1.9080173969268799\n",
      "steps: 34500 train loss: 1.9144583940505981 val loss: 1.906630277633667\n",
      "steps: 35000 train loss: 1.8990365266799927 val loss: 1.9126726388931274\n",
      "steps: 35500 train loss: 1.8969286680221558 val loss: 1.9003926515579224\n",
      "steps: 36000 train loss: 1.8871097564697266 val loss: 1.8955297470092773\n",
      "steps: 36500 train loss: 1.88840651512146 val loss: 1.9023377895355225\n",
      "steps: 37000 train loss: 1.8876104354858398 val loss: 1.8986884355545044\n",
      "steps: 37500 train loss: 1.8810546398162842 val loss: 1.8843207359313965\n",
      "steps: 38000 train loss: 1.9112519025802612 val loss: 1.895029902458191\n",
      "steps: 38500 train loss: 1.8818024396896362 val loss: 1.8754934072494507\n",
      "steps: 39000 train loss: 1.8899165391921997 val loss: 1.8825397491455078\n",
      "steps: 39500 train loss: 1.9090014696121216 val loss: 1.8978444337844849\n",
      "steps: 40000 train loss: 1.9040164947509766 val loss: 1.8784359693527222\n",
      "steps: 40500 train loss: 1.9119223356246948 val loss: 1.9170805215835571\n",
      "steps: 41000 train loss: 1.8853696584701538 val loss: 1.8870749473571777\n",
      "steps: 41500 train loss: 1.8889453411102295 val loss: 1.87517511844635\n",
      "steps: 42000 train loss: 1.866796851158142 val loss: 1.8781499862670898\n",
      "steps: 42500 train loss: 1.8718425035476685 val loss: 1.8818368911743164\n",
      "steps: 43000 train loss: 1.8745399713516235 val loss: 1.8685975074768066\n",
      "steps: 43500 train loss: 1.8636419773101807 val loss: 1.8719284534454346\n",
      "steps: 44000 train loss: 1.8768304586410522 val loss: 1.8758083581924438\n",
      "steps: 44500 train loss: 1.8820370435714722 val loss: 1.866451621055603\n",
      "steps: 45000 train loss: 1.8924190998077393 val loss: 1.881563663482666\n",
      "steps: 45500 train loss: 1.872342586517334 val loss: 1.8698886632919312\n",
      "steps: 46000 train loss: 1.8711601495742798 val loss: 1.855182409286499\n",
      "steps: 46500 train loss: 1.8581119775772095 val loss: 1.87860107421875\n",
      "steps: 47000 train loss: 1.8671904802322388 val loss: 1.870065689086914\n",
      "steps: 47500 train loss: 1.859670639038086 val loss: 1.874436855316162\n",
      "steps: 48000 train loss: 1.839475154876709 val loss: 1.8683379888534546\n",
      "steps: 48500 train loss: 1.8660541772842407 val loss: 1.8661201000213623\n",
      "steps: 49000 train loss: 1.8588380813598633 val loss: 1.855811595916748\n",
      "steps: 49500 train loss: 1.848911166191101 val loss: 1.8552507162094116\n",
      "steps: 50000 train loss: 1.8506923913955688 val loss: 1.8584480285644531\n",
      "steps: 50500 train loss: 1.847815752029419 val loss: 1.8609771728515625\n",
      "steps: 51000 train loss: 1.8769946098327637 val loss: 1.845958948135376\n",
      "steps: 51500 train loss: 1.851633071899414 val loss: 1.8610312938690186\n",
      "steps: 52000 train loss: 1.863213300704956 val loss: 1.8454138040542603\n",
      "steps: 52500 train loss: 1.8433735370635986 val loss: 1.8492860794067383\n",
      "steps: 53000 train loss: 1.8642513751983643 val loss: 1.8519748449325562\n",
      "steps: 53500 train loss: 1.8453549146652222 val loss: 1.8591976165771484\n",
      "steps: 54000 train loss: 1.8431150913238525 val loss: 1.8440186977386475\n",
      "steps: 54500 train loss: 1.8365371227264404 val loss: 1.8341280221939087\n",
      "steps: 55000 train loss: 1.862724781036377 val loss: 1.845692753791809\n",
      "steps: 55500 train loss: 1.8450475931167603 val loss: 1.8707760572433472\n",
      "steps: 56000 train loss: 1.841905117034912 val loss: 1.85329008102417\n",
      "steps: 56500 train loss: 1.8314074277877808 val loss: 1.8460166454315186\n",
      "steps: 57000 train loss: 1.8387514352798462 val loss: 1.857686996459961\n",
      "steps: 57500 train loss: 1.841557264328003 val loss: 1.8491177558898926\n",
      "steps: 58000 train loss: 1.8521760702133179 val loss: 1.8376578092575073\n",
      "steps: 58500 train loss: 1.8377714157104492 val loss: 1.8473519086837769\n",
      "steps: 59000 train loss: 1.850487470626831 val loss: 1.854215145111084\n",
      "steps: 59500 train loss: 1.8380334377288818 val loss: 1.8533786535263062\n",
      "steps: 60000 train loss: 1.8339338302612305 val loss: 1.838975191116333\n",
      "steps: 60500 train loss: 1.842625379562378 val loss: 1.846250295639038\n",
      "steps: 61000 train loss: 1.844596028327942 val loss: 1.8402934074401855\n",
      "steps: 61500 train loss: 1.8258880376815796 val loss: 1.828596830368042\n",
      "steps: 62000 train loss: 1.8569012880325317 val loss: 1.8476693630218506\n",
      "steps: 62500 train loss: 1.8411365747451782 val loss: 1.8337327241897583\n",
      "steps: 63000 train loss: 1.8218743801116943 val loss: 1.8232026100158691\n",
      "steps: 63500 train loss: 1.8550561666488647 val loss: 1.8494939804077148\n",
      "steps: 64000 train loss: 1.8299139738082886 val loss: 1.8191964626312256\n",
      "steps: 64500 train loss: 1.8161824941635132 val loss: 1.8243125677108765\n",
      "steps: 65000 train loss: 1.8300633430480957 val loss: 1.8519692420959473\n",
      "steps: 65500 train loss: 1.8228812217712402 val loss: 1.8303643465042114\n",
      "steps: 66000 train loss: 1.842422604560852 val loss: 1.84137761592865\n",
      "steps: 66500 train loss: 1.818429708480835 val loss: 1.8319966793060303\n",
      "steps: 67000 train loss: 1.81471848487854 val loss: 1.8304815292358398\n",
      "steps: 67500 train loss: 1.8240526914596558 val loss: 1.8335328102111816\n",
      "steps: 68000 train loss: 1.8392163515090942 val loss: 1.8331667184829712\n",
      "steps: 68500 train loss: 1.8204315900802612 val loss: 1.836889386177063\n",
      "steps: 69000 train loss: 1.8074471950531006 val loss: 1.815825343132019\n",
      "steps: 69500 train loss: 1.815530776977539 val loss: 1.8289647102355957\n",
      "steps: 70000 train loss: 1.8334460258483887 val loss: 1.817918300628662\n",
      "steps: 70500 train loss: 1.823782205581665 val loss: 1.8331044912338257\n",
      "steps: 71000 train loss: 1.8393863439559937 val loss: 1.8285248279571533\n",
      "steps: 71500 train loss: 1.826655387878418 val loss: 1.8217682838439941\n",
      "steps: 72000 train loss: 1.8289153575897217 val loss: 1.8382078409194946\n",
      "steps: 72500 train loss: 1.8355351686477661 val loss: 1.8176043033599854\n",
      "steps: 73000 train loss: 1.8253947496414185 val loss: 1.8106400966644287\n",
      "steps: 73500 train loss: 1.828688383102417 val loss: 1.8279160261154175\n",
      "steps: 74000 train loss: 1.8143506050109863 val loss: 1.8242170810699463\n",
      "steps: 74500 train loss: 1.8285393714904785 val loss: 1.8170642852783203\n",
      "steps: 75000 train loss: 1.799697995185852 val loss: 1.8168630599975586\n",
      "steps: 75500 train loss: 1.808898687362671 val loss: 1.8105063438415527\n",
      "steps: 76000 train loss: 1.818435549736023 val loss: 1.8070341348648071\n",
      "steps: 76500 train loss: 1.8088619709014893 val loss: 1.810336947441101\n",
      "steps: 77000 train loss: 1.8176618814468384 val loss: 1.8206125497817993\n",
      "steps: 77500 train loss: 1.8035656213760376 val loss: 1.8111755847930908\n",
      "steps: 78000 train loss: 1.810020923614502 val loss: 1.8148200511932373\n",
      "steps: 78500 train loss: 1.8114482164382935 val loss: 1.8020129203796387\n",
      "steps: 79000 train loss: 1.8162505626678467 val loss: 1.7923331260681152\n",
      "steps: 79500 train loss: 1.816228985786438 val loss: 1.8078887462615967\n",
      "steps: 80000 train loss: 1.8157260417938232 val loss: 1.8070149421691895\n",
      "steps: 80500 train loss: 1.8151365518569946 val loss: 1.8107012510299683\n",
      "steps: 81000 train loss: 1.7915005683898926 val loss: 1.801357388496399\n",
      "steps: 81500 train loss: 1.8146030902862549 val loss: 1.8029563426971436\n",
      "steps: 82000 train loss: 1.8099043369293213 val loss: 1.806386113166809\n",
      "steps: 82500 train loss: 1.8028264045715332 val loss: 1.8138371706008911\n",
      "steps: 83000 train loss: 1.808577060699463 val loss: 1.806085467338562\n",
      "steps: 83500 train loss: 1.8095577955245972 val loss: 1.807875156402588\n",
      "steps: 84000 train loss: 1.7895543575286865 val loss: 1.7949907779693604\n",
      "steps: 84500 train loss: 1.8056825399398804 val loss: 1.813387393951416\n",
      "steps: 85000 train loss: 1.8050895929336548 val loss: 1.8055644035339355\n",
      "steps: 85500 train loss: 1.8096587657928467 val loss: 1.8097507953643799\n",
      "steps: 86000 train loss: 1.7958203554153442 val loss: 1.8080581426620483\n",
      "steps: 86500 train loss: 1.7953532934188843 val loss: 1.784934639930725\n",
      "steps: 87000 train loss: 1.804742455482483 val loss: 1.7975140810012817\n",
      "steps: 87500 train loss: 1.795564889907837 val loss: 1.8170974254608154\n",
      "steps: 88000 train loss: 1.8157541751861572 val loss: 1.807612657546997\n",
      "steps: 88500 train loss: 1.7918274402618408 val loss: 1.789696455001831\n",
      "steps: 89000 train loss: 1.7955141067504883 val loss: 1.809314250946045\n",
      "steps: 89500 train loss: 1.788390874862671 val loss: 1.8095659017562866\n",
      "steps: 90000 train loss: 1.7983801364898682 val loss: 1.7881343364715576\n",
      "steps: 90500 train loss: 1.8026511669158936 val loss: 1.8069207668304443\n",
      "steps: 91000 train loss: 1.805984377861023 val loss: 1.806322455406189\n",
      "steps: 91500 train loss: 1.7763357162475586 val loss: 1.793840765953064\n",
      "steps: 92000 train loss: 1.7772092819213867 val loss: 1.8059674501419067\n",
      "steps: 92500 train loss: 1.7887084484100342 val loss: 1.77585768699646\n",
      "steps: 93000 train loss: 1.7973849773406982 val loss: 1.8020657300949097\n",
      "steps: 93500 train loss: 1.7801536321640015 val loss: 1.8033040761947632\n",
      "steps: 94000 train loss: 1.7920348644256592 val loss: 1.8058621883392334\n",
      "steps: 94500 train loss: 1.792065143585205 val loss: 1.8047986030578613\n",
      "steps: 95000 train loss: 1.7965682744979858 val loss: 1.7850033044815063\n",
      "steps: 95500 train loss: 1.7795522212982178 val loss: 1.7979131937026978\n",
      "steps: 96000 train loss: 1.794008493423462 val loss: 1.8057693243026733\n",
      "steps: 96500 train loss: 1.777530550956726 val loss: 1.781507134437561\n",
      "steps: 97000 train loss: 1.7890982627868652 val loss: 1.8011597394943237\n",
      "steps: 97500 train loss: 1.7796390056610107 val loss: 1.802770733833313\n",
      "steps: 98000 train loss: 1.8088434934616089 val loss: 1.7939144372940063\n",
      "steps: 98500 train loss: 1.7975282669067383 val loss: 1.7714622020721436\n",
      "steps: 99000 train loss: 1.7951892614364624 val loss: 1.7897287607192993\n",
      "steps: 99500 train loss: 1.7971464395523071 val loss: 1.7923126220703125\n",
      "2.2103636264801025\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"steps: {iter} train loss: {losses['train']} val loss: {losses['val']}\")\n",
    "        \n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 726 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_word = model.generate(context, max_new_tokens=50)\n",
    "seq = \" Batman is the \"\n",
    "for i in generated_word.tolist():\n",
    "    seq = seq + \"\"+tokenizer.detokenize(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Batman is the anged thee beimpey ftowistowegeepoweendered boreqrt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"artifacts\\\\moe_letter.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l2rpn-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
